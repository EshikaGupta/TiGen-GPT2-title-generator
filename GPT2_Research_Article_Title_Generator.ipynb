{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZMueBk0CAQL"
      },
      "source": [
        "##Retrain GPT-2 and Generate Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi3K-mKOBl8B",
        "outputId": "8f82f492-c02e-41f6-a717-0231d3996d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "!pip install -q gpt-2-simple\n",
        "!pip install -q arxivscraper\n",
        "!pip install rouge_score\n",
        "!pip install arxiv\n",
        "from rouge_score import rouge_scorer\n",
        "import gpt_2_simple as gpt2\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arxivscraper as ax\n",
        "import csv\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "#from tensorflow.keras import regularizers\n",
        "#import tensorflow.keras.utils as ku\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaqEs_8CEyi"
      },
      "source": [
        "###GPU\n",
        "Verify which GPU is active by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EKskYFsCCb3",
        "outputId": "5d466da4-e5b8-44a2-ce16-49088c0ae8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Sep 29 11:26:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aYK91AiCL9n"
      },
      "source": [
        "###Scrape data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiUXUiSdpa2A"
      },
      "source": [
        "Don't run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq_21h08-hsi",
        "outputId": "c373a3ea-c48a-4ac8-ff56-9803a4668a81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-fe266fa8d79b>:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for results in search.results():\n"
          ]
        }
      ],
      "source": [
        "import arxiv\n",
        "\n",
        "titles = []\n",
        "abstracts = []\n",
        "\n",
        "search = arxiv.Search(\n",
        "  query = \"nlp\",\n",
        "  max_results = 1000,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "for results in search.results():\n",
        "  titles.append(results.title)\n",
        "  abstracts.append(results.summary)\n",
        "\n",
        "data = {'titles':titles,'abstracts':abstracts}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('abs.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tURR8x3b03V4",
        "outputId": "6a2e0ad6-2bef-4f92-9cbc-322d750dec2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Scalable and Domain-General Abstractive Proposition Segmentation\",\n          \"Large Language Models: A New Approach for Privacy Policy Analysis at Scale\",\n          \"It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstracts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Segmenting text into fine-grained units of meaning is important to a wide\\nrange of NLP applications. The default approach of segmenting text into\\nsentences is often insufficient, especially since sentences are usually complex\\nenough to include multiple units of meaning that merit separate treatment in\\nthe downstream task. We focus on the task of abstractive proposition\\nsegmentation: transforming text into simple, self-contained, well-formed\\nsentences. Several recent works have demonstrated the utility of proposition\\nsegmentation with few-shot prompted LLMs for downstream tasks such as\\nretrieval-augmented grounding and fact verification. However, this approach\\ndoes not scale to large amounts of text and may not always extract all the\\nfacts from the input text. In this paper, we first introduce evaluation metrics\\nfor the task to measure several dimensions of quality. We then propose a\\nscalable, yet accurate, proposition segmentation model. We model proposition\\nsegmentation as a supervised task by training LLMs on existing annotated\\ndatasets and show that training yields significantly improved results. We\\nfurther show that by using the fine-tuned LLMs as teachers for annotating large\\namounts of multi-domain synthetic distillation data, we can train smaller\\nstudent models with results similar to the teacher LLMs. We then demonstrate\\nthat our technique leads to effective domain generalization, by annotating data\\nin two domains outside the original training data and evaluating on them.\\nFinally, as a key contribution of the paper, we share an easy-to-use API for\\nNLP practitioners to use.\",\n          \"The number and dynamic nature of web and mobile applications presents\\nsignificant challenges for assessing their compliance with data protection\\nlaws. In this context, symbolic and statistical Natural Language Processing\\n(NLP) techniques have been employed for the automated analysis of these\\nsystems' privacy policies. However, these techniques typically require\\nlabor-intensive and potentially error-prone manually annotated datasets for\\ntraining and validation. This research proposes the application of Large\\nLanguage Models (LLMs) as an alternative for effectively and efficiently\\nextracting privacy practices from privacy policies at scale. Particularly, we\\nleverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the\\noptimal design of prompts, parameters, and models, incorporating advanced\\nstrategies such as few-shot learning. We further illustrate its capability to\\ndetect detailed and varied privacy practices accurately. Using several renowned\\ndatasets in the domain as a benchmark, our evaluation validates its exceptional\\nperformance, achieving an F1 score exceeding 93%. Besides, it does so with\\nreduced costs, faster processing times, and fewer technical knowledge\\nrequirements. Consequently, we advocate for LLM-based solutions as a sound\\nalternative to traditional NLP techniques for the automated analysis of privacy\\npolicies at scale.\",\n          \"Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from\\ntextual data about specific entities and their corresponding aspects through\\nvarious complementary subtasks. Several prior research has focused on\\ndeveloping ad hoc designs of varying complexities for these subtasks. In this\\npaper, we present a generative framework extensible to any ABSA subtask. We\\nbuild upon the instruction tuned model proposed by Scaria et al. (2023), who\\npresent an instruction-based model with task descriptions followed by\\nin-context examples on ABSA subtasks. We propose PFInstruct, an extension to\\nthis instruction learning paradigm by appending an NLP-related task prefix to\\nthe task description. This simple approach leads to improved performance across\\nall tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the\\nATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average\\nof +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact\\nof the prefix-enhanced prompt quality on the ABSA subtasks and find that even a\\nnoisy prefix enhances model performance compared to the baseline. Our method\\nalso achieves competitive results on a biomedical domain dataset (ERSA).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a179af15-27b6-4fbe-bf93-3caae1dadded\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>abstracts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Auto-Demo Prompting: Leveraging Generated Outp...</td>\n",
              "      <td>Batch prompting is a common technique in large...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Seeing Eye to AI: Human Alignment via Gaze-Bas...</td>\n",
              "      <td>Advancements in Natural Language Processing (N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>InfiniPot: Infinite Context Processing on Memo...</td>\n",
              "      <td>Handling long input contexts remains a signifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Analyzing Byte-Pair Encoding on Monophonic and...</td>\n",
              "      <td>Byte-Pair Encoding (BPE) is an algorithm commo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AHP-Powered LLM Reasoning for Multi-Criteria E...</td>\n",
              "      <td>Question answering (QA) tasks have been extens...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Fusion of Domain-Adapted Vision and Language M...</td>\n",
              "      <td>Vision-language models, while effective in gen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Lessons from the Use of Natural Language Infer...</td>\n",
              "      <td>We investigate the use of Natural Language Inf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Towards a Holistic Evaluation of LLMs on Factu...</td>\n",
              "      <td>Large language models (LLMs) have shown remark...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Chat2Scenario: Scenario Extraction From Datase...</td>\n",
              "      <td>The advent of Large Language Models (LLM) prov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Mamba-360: Survey of State Space Models as Tra...</td>\n",
              "      <td>Sequence modeling is a crucial area across var...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a179af15-27b6-4fbe-bf93-3caae1dadded')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a179af15-27b6-4fbe-bf93-3caae1dadded button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a179af15-27b6-4fbe-bf93-3caae1dadded');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2be2fb28-90f4-46e4-87d9-d341fe66ed60\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2be2fb28-90f4-46e4-87d9-d341fe66ed60')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2be2fb28-90f4-46e4-87d9-d341fe66ed60 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ce6d416e-16e8-4b75-8884-e59642ce9d4c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ce6d416e-16e8-4b75-8884-e59642ce9d4c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                titles  \\\n",
              "0    Auto-Demo Prompting: Leveraging Generated Outp...   \n",
              "1    Seeing Eye to AI: Human Alignment via Gaze-Bas...   \n",
              "2    InfiniPot: Infinite Context Processing on Memo...   \n",
              "3    Analyzing Byte-Pair Encoding on Monophonic and...   \n",
              "4    AHP-Powered LLM Reasoning for Multi-Criteria E...   \n",
              "..                                                 ...   \n",
              "995  Fusion of Domain-Adapted Vision and Language M...   \n",
              "996  Lessons from the Use of Natural Language Infer...   \n",
              "997  Towards a Holistic Evaluation of LLMs on Factu...   \n",
              "998  Chat2Scenario: Scenario Extraction From Datase...   \n",
              "999  Mamba-360: Survey of State Space Models as Tra...   \n",
              "\n",
              "                                             abstracts  \n",
              "0    Batch prompting is a common technique in large...  \n",
              "1    Advancements in Natural Language Processing (N...  \n",
              "2    Handling long input contexts remains a signifi...  \n",
              "3    Byte-Pair Encoding (BPE) is an algorithm commo...  \n",
              "4    Question answering (QA) tasks have been extens...  \n",
              "..                                                 ...  \n",
              "995  Vision-language models, while effective in gen...  \n",
              "996  We investigate the use of Natural Language Inf...  \n",
              "997  Large language models (LLMs) have shown remark...  \n",
              "998  The advent of Large Language Models (LLM) prov...  \n",
              "999  Sequence modeling is a crucial area across var...  \n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLhz_7OfhT5B",
        "outputId": "e242f7ce-3a03-44b0-9225-26d96831cc87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUjtwUwhCSUs"
      },
      "source": [
        "###Fine-Tune GPT-2\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "124M (default): the \"small\" model, 500MB on disk.\n",
        "355M: the \"medium\" model, 1.5GB on disk.\n",
        "774M: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "1558M: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like 774M, it cannot be finetuned).\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing model_name in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at /models/<model_name>.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PGuG8rpCWvQ",
        "outputId": "0fb0b010-76d6-4b85-d53a-0647c036901c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 5.75Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.85Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 6.39Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:22, 63.8Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 1.47Git/s]                                               \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.61Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.92Mit/s]\n"
          ]
        }
      ],
      "source": [
        "model_name = \"355M\"\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hK3AaVXCYQC"
      },
      "source": [
        "###Mounting Google Drive\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model out of Colaboratory, is to route it through Google Drive first.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkH2u0ypCd3Q",
        "outputId": "958ce983-b1b2-4e2b-9bd3-2170ceb7c6ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "gpt2.mount_gdrive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eHm1MuiPBK85"
      },
      "outputs": [],
      "source": [
        "file_name2 = \"abs.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HMUD_MlNITL1"
      },
      "outputs": [],
      "source": [
        "!mv abs.csv drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vcI2Gq77CtJp"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_file_from_gdrive(file_name2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ilC7tB4TXFlf"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "with open(file_name2, 'r') as csvfile:\n",
        "  datareader = csv.reader(csvfile)\n",
        "  header = next(datareader)\n",
        "  for row in datareader:\n",
        "        a='[ABSTRACT]:'+row[1]\n",
        "        b='[TITLES]:'+row[0]\n",
        "        data.append(a)\n",
        "        data.append(b)\n",
        "\n",
        "with open('chatbot.txt','w') as f:\n",
        "  for line in data:\n",
        "    try:\n",
        "      f.write(line)\n",
        "      f.write('\\n')\n",
        "    except:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9378CXWPCwER"
      },
      "source": [
        "###Finetune GPT-2\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of steps. (to have the finetuning run indefinitely, set steps = -1)\n",
        "\n",
        "The model checkpoints will be saved in /checkpoint/run1 by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "Other optional-but-helpful parameters for gpt2.finetune:\n",
        "\n",
        "* restore_from: Set to fresh to start training from the base GPT-2, or set to latest to restart training from an existing checkpoint.\n",
        "* sample_every: Number of steps to print example output\n",
        "* print_every: Number of steps to print training progress.\n",
        "* learning_rate: Learning rate for the training. (default 1e-4, can lower to 1e-5 if you have <1MB input data)\n",
        "* run_name: subfolder within checkpoint to save the model. This is useful if you want to work with multiple models (will also need to specify run_name when loading the model)\n",
        "* overwrite: Set to True if you want to continue finetuning an existing model (w/ restore_from='latest') without creating duplicate copies.\n",
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a .rar compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovj2eexsXma2",
        "outputId": "28f1bf3e-0fdf-47b8-cf01-e45a0bd29303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'chatbot_poem.txt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "file_name='chatbot.txt'\n",
        "!mv chatbot_poem.txt drive/MyDrive\n",
        "gpt2.copy_file_from_gdrive(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ExkUvYJ4inUR"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SzKCxc1kg0sx"
      },
      "outputs": [],
      "source": [
        "sess = gpt2.start_tf_sess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwB-R-JUCxwl",
        "outputId": "e25671eb-49e3-45ae-b082-9db5b5b32b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For larger models, the recommended finetune() parameters are:\n",
            "\tuse_memory_saving_gradients = True\n",
            "\tonly_train_transformer_layers = True\n",
            "\taccumulate_gradients = 1\n",
            "\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset has 200888 tokens\n",
            "Training...\n",
            "[1 | 14.50] loss=3.13 avg=3.13\n",
            "[2 | 23.52] loss=3.30 avg=3.21\n",
            "[3 | 32.05] loss=3.34 avg=3.25\n",
            "[4 | 40.50] loss=3.39 avg=3.29\n",
            "[5 | 49.10] loss=3.25 avg=3.28\n",
            "[6 | 57.58] loss=3.24 avg=3.27\n",
            "[7 | 66.11] loss=3.37 avg=3.29\n",
            "[8 | 74.66] loss=3.10 avg=3.26\n",
            "[9 | 83.27] loss=2.90 avg=3.22\n",
            "[10 | 92.25] loss=3.16 avg=3.22\n",
            "[11 | 100.89] loss=2.78 avg=3.17\n",
            "[12 | 109.30] loss=3.02 avg=3.16\n",
            "[13 | 117.94] loss=3.27 avg=3.17\n",
            "[14 | 126.48] loss=2.89 avg=3.15\n",
            "[15 | 134.96] loss=3.12 avg=3.15\n",
            "[16 | 143.42] loss=2.81 avg=3.12\n",
            "[17 | 151.87] loss=2.88 avg=3.11\n",
            "[18 | 160.44] loss=3.02 avg=3.10\n",
            "[19 | 169.02] loss=3.09 avg=3.10\n",
            "[20 | 177.53] loss=3.31 avg=3.11\n",
            "[21 | 186.03] loss=3.05 avg=3.11\n",
            "[22 | 194.50] loss=3.05 avg=3.11\n",
            "[23 | 202.94] loss=3.19 avg=3.11\n",
            "[24 | 211.47] loss=3.28 avg=3.12\n",
            "[25 | 219.99] loss=2.98 avg=3.11\n",
            "[26 | 228.48] loss=2.98 avg=3.11\n",
            "[27 | 237.04] loss=2.83 avg=3.10\n",
            "[28 | 245.57] loss=3.10 avg=3.10\n",
            "[29 | 254.09] loss=2.30 avg=3.06\n",
            "[30 | 262.71] loss=2.62 avg=3.05\n",
            "[31 | 271.17] loss=3.00 avg=3.05\n",
            "[32 | 279.60] loss=2.70 avg=3.03\n",
            "[33 | 288.07] loss=2.45 avg=3.01\n",
            "[34 | 296.52] loss=2.88 avg=3.01\n",
            "[35 | 304.99] loss=2.94 avg=3.01\n",
            "[36 | 313.50] loss=2.82 avg=3.00\n",
            "[37 | 321.92] loss=3.01 avg=3.00\n",
            "[38 | 330.34] loss=3.13 avg=3.00\n",
            "[39 | 338.79] loss=2.92 avg=3.00\n",
            "[40 | 347.29] loss=2.98 avg=3.00\n",
            "[41 | 355.73] loss=3.34 avg=3.01\n",
            "[42 | 364.17] loss=3.36 avg=3.02\n",
            "[43 | 372.61] loss=3.21 avg=3.03\n",
            "[44 | 381.09] loss=3.06 avg=3.03\n",
            "[45 | 389.44] loss=2.91 avg=3.02\n",
            "[46 | 397.97] loss=2.83 avg=3.02\n",
            "[47 | 406.41] loss=2.41 avg=3.00\n",
            "[48 | 414.95] loss=2.35 avg=2.99\n",
            "[49 | 423.55] loss=2.94 avg=2.98\n",
            "[50 | 431.94] loss=2.72 avg=2.98\n",
            "======== SAMPLE 1 ========\n",
            " 2.9 percent of total emissions. The data used to predict the number of emissions in the next 1,000 years was also collected on a large number of countries, regions and cities, while providing the most appropriate scenario to evaluate the prediction accuracy. This study highlights the need for more detailed simulations, and our findings strongly inform future climate modeling efforts which are often focused on modeling uncertainty for future climate changes.\n",
            "[TITLES]:Are We Cooler Now?, The Future of Climate Prediction\n",
            "[ABSTRACT]:Increasingly, the development of climate models has been targeted towards addressing the most pressing issues in the real-world. Recently, an increasing number of applications have been developed for climate model development to address real-world challenges. Most of these applications are often focused on the transportation sector, and therefore the development of transportation-specific knowledge, models, and applications can also address critical applications. In this paper, we present a novel approach that is designed to provide a universal platform for developing and evaluating transportation-specific knowledge, models and applications within the context of major problems presented by transport networks and infrastructure, such as: transportation safety, traffic congestion, and related infrastructure. This approach aims to address the fundamental challenges related to transportation network design, optimization, and assessment by using a large-scale infrastructure dataset; moreover, it introduces the need for the exploration of a wide variety of applications\n",
            "[TITLES]:Understanding the Transportation Environment: A Universal Platform for Evaluating Transport Knowledge, Models, and Applications\n",
            "[ABSTRACT]:Human beings are very curious, passionate people and the desire to improve themselves, often leads to self-directed behavior. For example, the urge to solve puzzles is a powerful motivator – even if the results are not immediately obvious or predictable, we often learn to\n",
            "interpret them according to the needs and preferences of others. In this paper, we develop an approach to\n",
            "analyze the motives of individuals to solve specific problem and offer a generalized framework for\n",
            "evaluating the motivation of problem-solving strategies. This framework comprises a\n",
            "monomorphic method, an inductive method, a probabilistic method\n",
            "and a probabilistic regression metric. Our work is based on an experimental\n",
            "study in which the subjects' motivation was evaluated by the task of\n",
            "rewarding them with a question. Results demonstrate that the\n",
            "questionaire can capture the individual motivations of researchers and\n",
            "offer an effective tool for evaluating the motivation of problem-solving\n",
            "techniques. Our results also demonstrate that our method can be generalized to\n",
            "a broader set of problems, from public health to transportation.\n",
            "[TITLES]:Sentencing by Reason: On Reason and Emotions as a Link in Sentencing\n",
            "[ABSTRACT]:Answering a question of research-related content, we first examine the strengths and\n",
            "weaknesses of existing methods; and we then propose a solution to the current\n",
            "issue, with our proposal integrating some of the strengths, strengths and\n",
            "weaknesses of current approaches into a more appropriate\n",
            "context. Specifically, we define a new \"ask\" object, and we introduce the\n",
            "knowledge-based Knowledge-Based Knowledge Learning classifier, for which we\n",
            "develop an initial and extended framework based on natural language processing\n",
            "(NLP), a common method of using natural language processing (NLP), to\n",
            "assess scientific topics. We provide a formal model and a detailed\n",
            "evaluation of the model, and conclude that the model can be used to\n",
            "answer questions of any scientific or technical nature without any prior knowledge of\n",
            "the topics; that is to say, the class can be used to evaluate any scientific or\n",
            "technical issue related to scientific or technical content, using current methods\n",
            "of evaluating scientific content, not relying solely on an NLP-based model and a\n",
            "knowledge-based Knowledge Learning classifier.\n",
            "[TITLES]:Classifying Science Content\n",
            "[ABSTRACT]:The application of quantum computers has allowed the development of new\n",
            "information processing capabilities, including NLP-based reasoning and\n",
            "pre-trained linguistic models. In this paper, we propose a novel\n",
            "approach to perform inference with quantum-trained \"quanta\", an\n",
            "academic-quality language model developed at the Université de Montréal.\n",
            "With quantum supercomputer-assisted \"QA\" software, and the quantum\n",
            "supervised state-of-the-art DeepNet, we introduce QA in quantum\n",
            "supervised quantum information retrieval (QER) and discuss the quantum\n",
            "supervised inference capabilities of quanta, such as multilingual understanding,\n",
            "translate-like semantics, and non-local data structures. With a\n",
            "broad range of QA systems currently in use, in-context, and\n",
            "on-machine learning, we show how quantum-supervised quantum\n",
            "supervision of QA is possible. Our approach uses the LSTM architecture of\n",
            "quantum learning, which is based on a combination of low-resolution\n",
            "resolution quantum images and coarse-tuned quantum\n",
            "information, and uses deep learning architectures like Kagg\n",
            "\n",
            "[51 | 548.89] loss=2.90 avg=2.98\n",
            "[52 | 557.84] loss=2.54 avg=2.97\n",
            "[53 | 566.56] loss=3.03 avg=2.97\n",
            "[54 | 575.33] loss=3.14 avg=2.97\n",
            "[55 | 584.16] loss=2.95 avg=2.97\n",
            "[56 | 592.82] loss=2.63 avg=2.96\n",
            "[57 | 601.32] loss=3.11 avg=2.97\n",
            "[58 | 609.93] loss=2.62 avg=2.96\n",
            "[59 | 618.56] loss=2.76 avg=2.95\n",
            "[60 | 627.24] loss=2.74 avg=2.95\n",
            "[61 | 636.10] loss=2.64 avg=2.94\n",
            "[62 | 644.81] loss=3.05 avg=2.94\n",
            "[63 | 653.43] loss=2.94 avg=2.94\n",
            "[64 | 662.02] loss=3.14 avg=2.95\n",
            "[65 | 670.70] loss=2.52 avg=2.94\n",
            "[66 | 679.36] loss=2.70 avg=2.93\n",
            "[67 | 687.95] loss=2.73 avg=2.93\n",
            "[68 | 696.63] loss=2.71 avg=2.93\n",
            "[69 | 705.34] loss=2.81 avg=2.92\n",
            "[70 | 713.96] loss=2.61 avg=2.92\n",
            "[71 | 722.63] loss=2.75 avg=2.91\n",
            "[72 | 731.37] loss=3.10 avg=2.92\n",
            "[73 | 739.98] loss=2.80 avg=2.92\n",
            "[74 | 748.63] loss=2.44 avg=2.91\n",
            "[75 | 757.30] loss=2.24 avg=2.89\n",
            "[76 | 765.86] loss=2.81 avg=2.89\n",
            "[77 | 774.48] loss=2.40 avg=2.88\n",
            "[78 | 783.04] loss=2.35 avg=2.87\n",
            "[79 | 791.67] loss=2.47 avg=2.87\n",
            "[80 | 800.31] loss=2.96 avg=2.87\n",
            "[81 | 808.88] loss=2.33 avg=2.86\n",
            "[82 | 817.51] loss=2.89 avg=2.86\n",
            "[83 | 826.09] loss=2.84 avg=2.86\n",
            "[84 | 834.88] loss=3.00 avg=2.86\n",
            "[85 | 843.50] loss=2.51 avg=2.85\n",
            "[86 | 852.07] loss=2.23 avg=2.84\n",
            "[87 | 860.66] loss=2.28 avg=2.83\n",
            "[88 | 869.28] loss=2.43 avg=2.83\n",
            "[89 | 878.00] loss=2.29 avg=2.82\n",
            "[90 | 886.61] loss=2.75 avg=2.82\n",
            "[91 | 895.24] loss=2.69 avg=2.81\n",
            "[92 | 903.89] loss=3.15 avg=2.82\n",
            "[93 | 912.52] loss=2.32 avg=2.81\n",
            "[94 | 921.10] loss=2.53 avg=2.81\n",
            "[95 | 929.72] loss=2.82 avg=2.81\n",
            "[96 | 938.27] loss=2.31 avg=2.80\n",
            "[97 | 947.00] loss=2.37 avg=2.79\n",
            "[98 | 955.62] loss=2.85 avg=2.79\n",
            "[99 | 964.33] loss=2.68 avg=2.79\n",
            "[100 | 973.05] loss=2.39 avg=2.79\n",
            "Saving checkpoint/run1/model-100\n",
            "======== SAMPLE 1 ========\n",
            " in these cases, but the ability of these models to recognize the patterns and their associated text was examined. The results show that with very few modifications, very popular NLP texts from literature datasets like Wikipedia are readily understood. While several models of existing systems cannot be directly applied to such datasets, these models are not limited to just LSTM-based models. Furthermore, the use of these models is restricted to large corpus papers (e.g., research papers) and not to some general-purpose model for large datasets. These findings demonstrate the potential of combining LL models with existing systems and the need for further experiments to further elucidate the capabilities of LL models and to further explore the generative applications of LL models.\n",
            "[TITLES]:Using LL models to Explain Large Language Models\n",
            "[ABSTRACT]:In-domain natural language processing (NLP) has shown great promise and is becoming the main goal of the future in many fields including healthcare. While advanced text classification methods like Large Language Models (LMs) have shown success in treating NLP data, data with ambiguous meaning has proved problematic as model performance drops due to the lack of semantic meaning.\n",
            "This paper focuses on the problems of ambiguity in NLP. We propose a method to eliminate ambiguity for in-domain NLP by applying a simple and adaptable model to a small group of ambiguous text. In-domain LMs can be trained on such texts or have to be directly connected between a single entity to have high performance. The methods used in this paper can be successfully applied to the ambiguity problem in multiple domains and methods have been described in detail before (such as the \"cluster-top-rule\" method). However, there is a challenge in applying the methods to the in-domain NLP: the ambiguity of in-domain models is a well-known problem. This paper addresses these challenges by proposing a new NLP domain-specific method for eliminating ambiguity on in-domain LMs that is based on an extractive method. Specifically, our method extracts multiple entities from the extracted text and applies these entities to a monolayer to generate an LSTM, thereby reducing the dependence of the extraction process on the presence of entities in the monolayer. The proposed results on multiple domains are well-suited to a general-purpose domain (e.g., healthcare) and demonstrate the capabilities of the in-domain LMs in their natural habitat. Our approach should prove to be useful for avoiding a range of ambiguities in in-domain NLP.\n",
            "[TITLES]:Unmasking Alignments for In-Domain Language Models\n",
            "[ABSTRACT]:The adoption of intelligent technologies and language understanding is an ever-changing trend. It often highlights the importance of natural language processing (NLP) technologies while emphasizing the fact that they have no formal training and are difficult to evaluate. In this paper, we first introduce the NLP Framework for Natural Language Processing. The framework is designed to be a collection of resources with a particular aim in this arena. Following the development of multiple models for text classification, two important questions are posed. How can we best apply natural language processing technologies to solve these issues? And what can we learn from the development of models? Several preliminary results show that the best approach is to use annotators for text classification and they are surprisingly effective. But when the model is tested on different text representations, it is not clear why it achieves better results than the natural models. We then ask the following question, \"Can we combine these results to optimize the model?\" Our initial proposal is an extensive approach towards a better understanding of how natural language processing technology works. Our results demonstrate that our approach can be applied in a domain where there is an imbalance of knowledge between models and other knowledge in the model.\n",
            "[TITLES]:Improving the Knowledge of Natural Language Processing\n",
            "[ABSTRACT]:The ability to analyze and interpret written text is fundamental for a wide range of human endeavors. In recent years, data analysis (DAA) has gained prominence in NLP and is becoming increasingly popular as a research goal. We present a new dataset for data analysis called the Knowledge Graph. This graph is composed of hundreds of annotated datasets with knowledge requirements and a large amount of extracted data. To analyze the knowledge requirement graph more deeply, We conduct a comprehensive literature search and conduct a number of experiments, including comparing the knowledge requirement graph with an alternative model, the RNG. Finally, we use advanced statistical methods to evaluate the existing knowledge graph and evaluate the knowledge requirement model in terms of its ability to capture knowledge about the relevant dataset. We show that the knowledge graph has the most beneficial attributes of any existing knowledge graph.\n",
            "[TITLES]:Ans: Answering questions from a knowledge graph\n",
            "[ABSTRACT]:When writing code, we encounter unexpected situations where we write unexpected code in our code base. At each stage, the code is subjected to numerous optimization attempts such as rewriting, refactoring, reorganization, etc. In this work, we present the first\n",
            "\n",
            "[101 | 1088.50] loss=2.35 avg=2.78\n",
            "[102 | 1097.23] loss=2.61 avg=2.78\n",
            "[103 | 1105.84] loss=2.66 avg=2.77\n",
            "[104 | 1114.58] loss=2.76 avg=2.77\n",
            "[105 | 1123.24] loss=3.00 avg=2.78\n",
            "[106 | 1131.83] loss=2.83 avg=2.78\n",
            "[107 | 1140.35] loss=2.81 avg=2.78\n",
            "[108 | 1148.89] loss=2.28 avg=2.77\n",
            "[109 | 1157.44] loss=2.26 avg=2.76\n",
            "[110 | 1165.93] loss=2.63 avg=2.76\n",
            "[111 | 1174.48] loss=2.73 avg=2.76\n",
            "[112 | 1183.04] loss=2.49 avg=2.76\n",
            "[113 | 1191.64] loss=2.42 avg=2.75\n",
            "[114 | 1200.23] loss=2.93 avg=2.75\n",
            "[115 | 1208.74] loss=2.35 avg=2.75\n",
            "[116 | 1217.39] loss=2.79 avg=2.75\n",
            "[117 | 1225.93] loss=2.88 avg=2.75\n",
            "[118 | 1234.64] loss=2.83 avg=2.75\n",
            "[119 | 1243.27] loss=2.81 avg=2.75\n",
            "[120 | 1251.76] loss=2.31 avg=2.75\n",
            "[121 | 1260.33] loss=1.77 avg=2.73\n",
            "[122 | 1268.87] loss=1.97 avg=2.72\n",
            "[123 | 1277.40] loss=3.09 avg=2.73\n",
            "[124 | 1285.97] loss=2.88 avg=2.73\n",
            "[125 | 1294.46] loss=1.91 avg=2.72\n",
            "[126 | 1303.02] loss=2.90 avg=2.72\n",
            "[127 | 1311.59] loss=2.58 avg=2.72\n",
            "[128 | 1320.17] loss=2.89 avg=2.72\n",
            "[129 | 1328.86] loss=2.29 avg=2.71\n",
            "[130 | 1337.36] loss=2.12 avg=2.71\n",
            "[131 | 1345.88] loss=2.79 avg=2.71\n",
            "[132 | 1354.50] loss=2.80 avg=2.71\n",
            "[133 | 1363.09] loss=2.75 avg=2.71\n",
            "[134 | 1371.71] loss=2.38 avg=2.71\n",
            "[135 | 1380.34] loss=2.52 avg=2.70\n",
            "[136 | 1388.96] loss=2.68 avg=2.70\n",
            "[137 | 1397.66] loss=2.11 avg=2.69\n",
            "[138 | 1406.26] loss=2.32 avg=2.69\n",
            "[139 | 1414.78] loss=3.20 avg=2.70\n",
            "[140 | 1423.46] loss=3.06 avg=2.70\n",
            "[141 | 1432.01] loss=2.47 avg=2.70\n",
            "[142 | 1440.55] loss=2.08 avg=2.69\n",
            "[143 | 1449.07] loss=3.17 avg=2.70\n",
            "[144 | 1457.63] loss=2.18 avg=2.69\n",
            "[145 | 1466.26] loss=3.00 avg=2.69\n",
            "[146 | 1474.90] loss=2.55 avg=2.69\n",
            "[147 | 1483.60] loss=2.96 avg=2.70\n",
            "[148 | 1492.13] loss=2.46 avg=2.69\n",
            "[149 | 1500.68] loss=2.15 avg=2.69\n",
            "[150 | 1509.39] loss=2.01 avg=2.68\n",
            "======== SAMPLE 1 ========\n",
            "seen data from a user\n",
            "logged into N-body (NBN) terminals to study the effects of these\n",
            "methodologies on understanding and summarizing the information contained in a real-world\n",
            "document. In this paper, we present a simple method to categorize the\n",
            "sentiment and text of N-body documents according to their sentiment and\n",
            "sentimentary content, and describe the methods used to categorize them.\n",
            "The proposed approach, which we call the Sentiment-Decision\n",
            "Categorization Algorithm (SDCA), employs a simple but effective strategy\n",
            "to extract sentiment from documents of varying sentiment levels and different types\n",
            "of information.\n",
            "[TITLES]:Sentiment Decategorization Algorithm\n",
            "[ABSTRACT]:This study provides a quantitative analysis of the impact of\n",
            "various tools in the legal sector on the accuracy of legal documents and\n",
            "data. Specifically, the study covers legal texts and includes multiple legal tools\n",
            "in combination with legal language, which are introduced. Furthermore, they are\n",
            "discussed in terms of their effect on human errors. This study also\n",
            "introduces a new tool to analyze the effects of legal tools on legal data. The\n",
            "analysis is conducted on the largest dataset (250K\n",
            "annotated).\n",
            "[TITLES]:Statistical Analys of Legal Tools and Legal Data Effects on Legal Knowledge\n",
            "[ABSTRACT]:We present a natural language-based search approach to identify the\n",
            "occurrence of a sentence in a document. Using the previous work, we first\n",
            "introduce two language models to analyze the sentence using the semantic\n",
            "model and a syntactic model. We then conduct a search using two alternative words to\n",
            "select the occurrence of such a semantic token; which leads us to a\n",
            "final search based on the two syntactic models. We show that both of\n",
            "them can achieve high level accuracy with our best model, which is compared to\n",
            "the best model that the best model previously produced the text, for\n",
            "instance, to identify the same token when it follows the singular and plural forms of the same\n",
            "sentence. Furthermore, this study shows that the syntactic model can outperform\n",
            "the semantic model for several natural language questions;\n",
            "the results show that the syntactic model leads to a higher accuracy while\n",
            "the semantic model only has an average accuracy.\n",
            "[TITLES]:Exploring the occurrence of a sentence in a document\n",
            "[ABSTRACT]:In the early years of machine translation, there were many attempts to improve the\n",
            "translation accuracy. However, it appears that the improvement has not yet reached the\n",
            "translator. In order to find new improvements, the authors propose a novel\n",
            "approach: pre-training a language model. Pre-training a language model is a\n",
            "method which uses pre-training examples to train a language model. Pre-training\n",
            "a language model is a common practice in the translators profession, but the\n",
            "practice is based on using large sets of examples, and is not\n",
            "as simple as using a simple model. In this study, we aim at improving the\n",
            "translation accuracy by using a large set of pre-trained language models.\n",
            "[TITLES]:Pre-training a Language Model to Improve Translation Accuracy\n",
            "[ABSTRACT]:Natural Language Processing (NLP) models have been widely used to analyse texts,\n",
            "either in- or out-of-domain: with the goal of generating information\n",
            "from the data. As a result of the existing advances, the\n",
            "in-domain and out-of-domain tasks have both been investigated. While a\n",
            "number of previous works have sought to apply NLP generalization to the\n",
            "out-of-domain settings, there is a still significant gap between\n",
            "the domains. Thus, in order to bridge the gap, an interdisciplinary\n",
            "research project is proposed: NLP generalization as a generalizable method\n",
            "for NLP tasks and domains. While most previous studies have explored\n",
            "generalizations of the domain (e.g., sentence-level generalization), their\n",
            "interpretation has been limited due to the limitations in the\n",
            "comprehension of sentence-level generalization. In addition, in particular\n",
            "cases, there is a need for more efficient implementations.\n",
            "[TITLES]:Linking Natural Language Processing Models to Out-of-Domain Text Generation\n",
            "[ABSTRACT]:We present a novel data augmentation approach for using a\n",
            "large-scale data set. We extend the previous approach by\n",
            "adding a data enhancement task, which introduces data\n",
            "unwrapping as the basis for data augmentation. By using this\n",
            "method, we train an efficient data augmentation model. The\n",
            "enhancement task uses two datasets, one for regularizes the\n",
            "data and another for augmenting the data. The effectiveness of the data\n",
            "enhancement approach in optimizing the performance of a\n",
            "wide range of datasets is demonstrated, including text classification\n",
            "(TextDeQ), NLP, text summarization (TextSM) and text\n",
            "concatenation (TextGPT).\n",
            "\n",
            "[151 | 1622.69] loss=1.73 avg=2.66\n",
            "[152 | 1631.61] loss=2.05 avg=2.66\n",
            "[153 | 1640.34] loss=2.23 avg=2.65\n",
            "[154 | 1649.01] loss=2.70 avg=2.65\n",
            "[155 | 1657.53] loss=2.15 avg=2.65\n",
            "[156 | 1666.30] loss=1.80 avg=2.63\n",
            "[157 | 1674.80] loss=2.43 avg=2.63\n",
            "[158 | 1683.38] loss=2.66 avg=2.63\n",
            "[159 | 1691.95] loss=2.03 avg=2.63\n",
            "[160 | 1700.52] loss=1.73 avg=2.61\n",
            "[161 | 1709.04] loss=2.05 avg=2.61\n",
            "[162 | 1717.64] loss=2.77 avg=2.61\n",
            "[163 | 1726.29] loss=1.75 avg=2.60\n",
            "[164 | 1734.88] loss=2.70 avg=2.60\n",
            "[165 | 1743.40] loss=3.03 avg=2.60\n",
            "[166 | 1752.12] loss=2.32 avg=2.60\n",
            "[167 | 1760.66] loss=2.99 avg=2.61\n",
            "[168 | 1769.29] loss=2.77 avg=2.61\n",
            "[169 | 1777.89] loss=1.75 avg=2.60\n",
            "[170 | 1786.40] loss=3.09 avg=2.60\n",
            "[171 | 1794.98] loss=3.06 avg=2.61\n",
            "[172 | 1803.61] loss=1.85 avg=2.60\n",
            "[173 | 1812.12] loss=2.54 avg=2.60\n",
            "[174 | 1820.65] loss=2.49 avg=2.60\n",
            "[175 | 1829.13] loss=2.58 avg=2.60\n",
            "[176 | 1837.73] loss=2.73 avg=2.60\n",
            "[177 | 1846.26] loss=2.76 avg=2.60\n",
            "[178 | 1854.80] loss=2.29 avg=2.60\n",
            "[179 | 1863.43] loss=2.05 avg=2.59\n",
            "[180 | 1872.05] loss=1.70 avg=2.58\n",
            "[181 | 1880.56] loss=3.11 avg=2.59\n",
            "[182 | 1889.16] loss=1.76 avg=2.58\n",
            "[183 | 1897.68] loss=2.77 avg=2.58\n",
            "[184 | 1906.18] loss=2.85 avg=2.58\n",
            "[185 | 1914.71] loss=2.55 avg=2.58\n",
            "[186 | 1923.21] loss=2.37 avg=2.58\n",
            "[187 | 1931.96] loss=2.93 avg=2.58\n",
            "[188 | 1940.48] loss=2.17 avg=2.58\n",
            "[189 | 1949.03] loss=2.65 avg=2.58\n",
            "[190 | 1957.53] loss=2.39 avg=2.58\n",
            "[191 | 1966.07] loss=2.62 avg=2.58\n",
            "[192 | 1974.66] loss=2.69 avg=2.58\n",
            "[193 | 1983.13] loss=1.76 avg=2.57\n",
            "[194 | 1991.66] loss=2.84 avg=2.57\n",
            "[195 | 2000.17] loss=2.96 avg=2.58\n",
            "[196 | 2008.71] loss=2.63 avg=2.58\n",
            "[197 | 2017.39] loss=2.68 avg=2.58\n",
            "[198 | 2025.89] loss=2.38 avg=2.58\n",
            "[199 | 2034.55] loss=2.18 avg=2.57\n",
            "[200 | 2043.05] loss=2.76 avg=2.57\n",
            "Saving checkpoint/run1/model-200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1067: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== SAMPLE 1 ========\n",
            " valuable (but not very much) and much simpler to comprehend than the original. As a result, the model performed at a level comparable to or better than existing large language models.\n",
            "[TITLES]:Towards a Large Language Model Learning System for Machine Translation\n",
            "[ABSTRACT]:With the advent of NLP, language learning has greatly facilitated the growth and adoption of new text processing technologies. Yet, as these technologies progress, they will introduce new challenges and, eventually, require large amounts of data. The problem is that we know that NLP is a probabilistic process; a process that relies on the data generated by the system. It is easy to understand why data is scarce; it is impossible to know how many data samples will exist in the future. We present LSTM, a probabilistic language model that exploits this problem and predicts a finite amount of data samples, e.g., a minimum, which are relevant for the task at hand. By applying LSTM to various text processing tasks, we can gauge the impact of LSTM on the development, testing and deployment of text processing pipelines and algorithms. We explore the impact of LSTM on the development of text-to-image and text-to-text models to capture text processing tasks in various domains. LSTM outperforms both of its trained competitors by 1.3 to 2.2 standard deviations in accuracy. Moreover, by modeling the model's task-integrated text results in a machine-evaluated prediction of the future samples of the sample, we show that LSTM's performance on the training set is equivalent to or exceeds that of a general linear regression model trained with the same set of examples alone. Our code is released at\n",
            "[\n",
            "https://github.com/zjunloung/LSTM\n",
            "\n",
            "\n",
            "[TITLES]:LSTM: How LSTM Gets Things Done Without Telling You\n",
            "[ABSTRACT]:In recent years, data mining has become the dominant technique for the systematic study of digital information. Data mining is a type of\n",
            "data mining, wherein a data set is randomly generated into an unsupervised\n",
            "state. For instance, if we randomly generate a list of 100 questions from a text\n",
            "scenario, these questions will be generated in the form of a text comprehension query. Due to\n",
            "the massive amount of data generated for each query, we observe that these data\n",
            "matters must be efficiently collected, processed, and analysed for efficient data\n",
            "mining experiments. Nevertheless, prior work fails to explain the behaviour of data\n",
            "mining for understanding the fundamental nature of data-mining experiments in\n",
            "natural language processing (NLP). This study seeks to do so by examining data processing\n",
            "exercises of a deep neural network (DNN) and an F1 classification model trained on the same\n",
            "data set; the latter learns to classify data according to the F1-classification\n",
            "probability distribution. Experimental results on a set of 10 datasets, across 25\n",
            "labels, demonstrate that when using data-mining-powered data processing models,\n",
            "data mining models do not work as advertised. Instead, their underlying\n",
            "systemic performance is significantly worse than using data-grubbing techniques.\n",
            "To the best of our knowledge, this is the first empirical study exploring\n",
            "how data mining techniques impact data processing operations at the fine-grained\n",
            "level.<|endoftext|>For years our models have relied on natural language processing (NLP) to capture\n",
            "text perception. These models have included large language models (LLMs) such as\n",
            "GPT-3. In this work, we discuss the shortcomings of these models and propose a new and\n",
            "intuitive approach to deal with them.\n",
            "[TITLES]:Saying it with the Models: A Case for Better Language Models\n",
            "[ABSTRACT]:This paper introduces a unified benchmark based on sentence-level data for task\n",
            "recognition and understanding the implications of the data. The idea is to use simple\n",
            "structures that can be understood for both general and specific tasks. For\n",
            "example, the simple structure of the data comprises of a word embeddings of\n",
            "two sentences, i.e. which are to be recognized and understood as two sentences,\n",
            "each containing a text segment which contains a token denoting the token being recognized as\n",
            "the referent and a token denoting the token being understood as the referent. We use GPT-3\n",
            "and AVID as examples, and include both large and small language models (LLMs)\n",
            "using these models. We experiment with different input structures, including large and\n",
            "light-weight. We find that GPT-3 performs even on complex sentences but fails to meet standard\n",
            "benchmarks for fine-grained sentence comprehension and comprehension of general\n",
            "sentences, while AVID has little if any ability to make these simple tasks easier. Our\n",
            "results demonstrate that AVID cannot achieve these simple benchmarks across a wide range\n",
            "of datasets and model variants.\n",
            "[TITLES]:Uniting Benchmark\n",
            "\n",
            "[201 | 2158.25] loss=2.67 avg=2.58\n",
            "[202 | 2166.92] loss=2.41 avg=2.57\n",
            "[203 | 2175.45] loss=2.76 avg=2.58\n",
            "[204 | 2184.15] loss=2.06 avg=2.57\n",
            "[205 | 2192.76] loss=2.41 avg=2.57\n",
            "[206 | 2201.35] loss=2.79 avg=2.57\n",
            "[207 | 2209.94] loss=1.95 avg=2.56\n",
            "[208 | 2218.67] loss=2.50 avg=2.56\n",
            "[209 | 2227.28] loss=2.94 avg=2.57\n",
            "[210 | 2235.85] loss=2.81 avg=2.57\n",
            "[211 | 2244.42] loss=1.85 avg=2.56\n",
            "[212 | 2253.01] loss=2.62 avg=2.56\n",
            "[213 | 2261.71] loss=2.83 avg=2.57\n",
            "[214 | 2270.37] loss=1.99 avg=2.56\n",
            "[215 | 2278.95] loss=2.57 avg=2.56\n",
            "[216 | 2287.59] loss=2.67 avg=2.56\n",
            "[217 | 2296.23] loss=2.55 avg=2.56\n",
            "[218 | 2304.71] loss=1.95 avg=2.55\n",
            "[219 | 2313.22] loss=1.95 avg=2.55\n",
            "[220 | 2321.76] loss=2.14 avg=2.54\n",
            "[221 | 2330.59] loss=2.31 avg=2.54\n",
            "[222 | 2339.12] loss=1.81 avg=2.53\n",
            "[223 | 2347.70] loss=2.32 avg=2.53\n",
            "[224 | 2356.23] loss=1.57 avg=2.52\n",
            "[225 | 2364.68] loss=2.46 avg=2.52\n",
            "[226 | 2373.29] loss=2.72 avg=2.52\n",
            "[227 | 2381.81] loss=2.13 avg=2.52\n",
            "[228 | 2390.40] loss=2.23 avg=2.51\n",
            "[229 | 2398.98] loss=2.22 avg=2.51\n",
            "[230 | 2407.61] loss=2.55 avg=2.51\n",
            "[231 | 2416.17] loss=2.42 avg=2.51\n",
            "[232 | 2424.78] loss=2.67 avg=2.51\n",
            "[233 | 2433.27] loss=1.87 avg=2.50\n",
            "[234 | 2441.83] loss=2.00 avg=2.50\n",
            "[235 | 2450.34] loss=2.27 avg=2.49\n",
            "[236 | 2459.02] loss=2.21 avg=2.49\n",
            "[237 | 2467.57] loss=2.69 avg=2.49\n",
            "[238 | 2475.99] loss=1.43 avg=2.48\n",
            "[239 | 2484.58] loss=2.94 avg=2.49\n",
            "[240 | 2493.15] loss=2.13 avg=2.48\n",
            "[241 | 2501.70] loss=1.65 avg=2.47\n",
            "[242 | 2510.35] loss=1.75 avg=2.47\n",
            "[243 | 2518.92] loss=1.82 avg=2.46\n",
            "[244 | 2527.53] loss=2.63 avg=2.46\n",
            "[245 | 2536.14] loss=1.78 avg=2.45\n",
            "[246 | 2544.66] loss=1.74 avg=2.45\n",
            "[247 | 2553.31] loss=2.05 avg=2.44\n",
            "[248 | 2561.87] loss=2.26 avg=2.44\n",
            "[249 | 2570.39] loss=2.36 avg=2.44\n",
            "[250 | 2578.97] loss=3.35 avg=2.45\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "[TITLES]:Citrine: Data Mining for Understanding the Political Landscape Using Large Language Models\n",
            "[ABSTRACT]:The recent development of large language models (LLMs) such as GPT3 has been accompanied by a\n",
            "growing number of articles addressing the issue of embodied generative models. We\n",
            "persist in arguing that \"natural language\" always makes little headway in addressing\n",
            "embodied generative tasks, in addition to its inherent difficulties in representing\n",
            "complex ideas in such limited form. This article introduces a new category\n",
            "of embodied generative tasks called \"sentiment analysis\" which attempts to\n",
            "explore how individuals' attitudes and beliefs affect their behaviour. With\n",
            "this paper, we tackle one of these difficult problems by creating new LLMs\n",
            "called SentiGUID, based on an embodied generative framework. The main difference between\n",
            "existing LLMs and the SentiGUID model is its reliance on knowledge from the\n",
            "sentiment-generating corpus. We show that SentiGUID performs significantly worse\n",
            "on sentiment analyses than the general LLMP and the same cannot be said for\n",
            "general Sentiment Analysis, which performs significantly better than LLMs. Our\n",
            "experiments on four sentiment analysis datasets show that using LLM can\n",
            "significantly improve the LLM's performance and the findings are discussed in\n",
            "full depth.\n",
            "[TITLES]:Sentiment Analysis with Large Language Models\n",
            "[ABSTRACT]:The increasing number of articles on NLP conveying personal information via\n",
            "deep neural networks (DNNs) shows that natural language processing (NLP)\n",
            "research is progressing rapidly, both within the field of NLP and across NLP\n",
            "tasks. In this paper, we introduce the first comprehensive survey of natural\n",
            "language processing with regards to personal information conveyance tasks. In this\n",
            "survey, we present the first comprehensive compilation of research papers on\n",
            "personal information conveance tasks across NLP, and discuss the key findings\n",
            "from the survey. We explain our compilation methodology and propose the goal\n",
            "of our survey as follows: We collected research papers from the literature search engine Nucleotide,\n",
            "and extracted the tasks under NLP and personal information conveance. We then summarized these\n",
            "tasks by a theme, which covers the NLP tasks and the Nucleotide tasks that\n",
            "involved DNNs. We identified the major issues within NLP research as they relate to\n",
            "personal information conveance tasks. We also identified noteworthy NLP research\n",
            "papers from different subfields. Finally, we discuss the implications of our\n",
            "surge in NLP research and present recommendations for future work on this topic.\n",
            "[TITLES]:NLP with a View: Personal Information and Language Representations in Natural Language Processing\n",
            "[ABSTRACT]:Languages can express multiple concepts through verbal expression. However, it is\n",
            "not always obvious to speakers what exactly those concepts are. For a language\n",
            "to be able to express complex concepts, for example, they need a rich vocabulary\n",
            "that allows them to express their thoughts. However, a language\n",
            "that knows how to express complex concepts in verbal expression would need to perform\n",
            "inference task that understands the conceptualization process of a language. We\n",
            "present an NLP task that performs empirical investigation into the construct\n",
            "complexity of various DNN-based talkative systems from different domains,\n",
            "e.g. education, economics, management, etc. Our results show that the most\n",
            "complex systems exhibited highest complexity of conceptualization process when\n",
            "asked to express complex thoughts in English. Our study suggests that the reasoning process\n",
            "from language to form an implicit framework and that the complexity of formulating\n",
            "complex thoughts in English increases with domain knowledge.\n",
            "[TITLES]:What Is the Complexity of Thought in English?\n",
            "[ABSTRACT]:As natural language understanding (NLE) has shown to be a highly challenging task, it is\n",
            "not surprising that many studies have been conducted to address the problem in non-English\n",
            "languages. However, these studies mostly focus on English, despite their\n",
            "considerable progress in other languages. In this paper, we review a series of\n",
            "emerging research questions on NLE that have been addressed using recent\n",
            "NLP techniques in non-English languages such as Swahili, Creole, and\n",
            "Congolese. The key findings are that although using NLP techniques does not\n",
            "completely address the NLE challenge in NLP-intensive languages like Swahili, a simple\n",
            "modifying task can overcome the challenges. This work shows how data\n",
            "sampling can be used to identify NLE challenges in languages that\n",
            "have seen modest progress, and how the use of semantic knowledge in the\n",
            "model-neutral domain can help overcome language\n",
            "difficulty. The limitations of current data formats make it impractical\n",
            "for large language models (LLMs) to be collected from a general\n",
            "language resource, but it is possible to augment the\n",
            "data to support NLE tasks. We highlight the importance of considering\n",
            "data distribution as critical for N\n",
            "\n",
            "[251 | 2691.16] loss=1.15 avg=2.43\n",
            "[252 | 2700.24] loss=1.84 avg=2.43\n",
            "[253 | 2709.10] loss=1.12 avg=2.41\n",
            "[254 | 2717.75] loss=2.04 avg=2.41\n",
            "[255 | 2726.43] loss=1.85 avg=2.40\n",
            "[256 | 2735.29] loss=2.33 avg=2.40\n",
            "[257 | 2744.13] loss=1.91 avg=2.40\n",
            "[258 | 2752.84] loss=2.11 avg=2.39\n",
            "[259 | 2761.45] loss=2.56 avg=2.40\n",
            "[260 | 2770.21] loss=2.31 avg=2.40\n",
            "[261 | 2778.86] loss=1.93 avg=2.39\n",
            "[262 | 2787.53] loss=2.51 avg=2.39\n",
            "[263 | 2796.23] loss=2.62 avg=2.39\n",
            "[264 | 2804.85] loss=2.69 avg=2.40\n",
            "[265 | 2813.53] loss=2.06 avg=2.39\n",
            "[266 | 2822.20] loss=2.24 avg=2.39\n",
            "[267 | 2830.85] loss=2.43 avg=2.39\n",
            "[268 | 2839.56] loss=2.00 avg=2.39\n",
            "[269 | 2848.21] loss=1.87 avg=2.38\n",
            "[270 | 2856.87] loss=1.76 avg=2.38\n",
            "[271 | 2865.55] loss=3.03 avg=2.38\n",
            "[272 | 2874.16] loss=2.11 avg=2.38\n",
            "[273 | 2882.82] loss=1.83 avg=2.37\n",
            "[274 | 2891.52] loss=1.82 avg=2.37\n",
            "[275 | 2900.16] loss=2.28 avg=2.37\n",
            "[276 | 2908.84] loss=2.04 avg=2.36\n",
            "[277 | 2917.45] loss=2.69 avg=2.37\n",
            "[278 | 2926.06] loss=3.07 avg=2.37\n",
            "[279 | 2934.74] loss=2.93 avg=2.38\n",
            "[280 | 2943.44] loss=2.13 avg=2.38\n",
            "[281 | 2952.10] loss=2.51 avg=2.38\n",
            "[282 | 2960.79] loss=2.51 avg=2.38\n",
            "[283 | 2969.43] loss=1.60 avg=2.37\n",
            "[284 | 2978.09] loss=1.74 avg=2.37\n",
            "[285 | 2986.91] loss=1.91 avg=2.36\n",
            "[286 | 2995.55] loss=1.56 avg=2.35\n",
            "[287 | 3004.21] loss=1.46 avg=2.34\n",
            "[288 | 3012.85] loss=2.91 avg=2.35\n",
            "[289 | 3021.75] loss=2.30 avg=2.35\n",
            "[290 | 3030.64] loss=1.56 avg=2.34\n",
            "[291 | 3039.31] loss=2.45 avg=2.34\n",
            "[292 | 3048.08] loss=1.44 avg=2.33\n",
            "[293 | 3057.01] loss=1.71 avg=2.33\n",
            "[294 | 3065.70] loss=1.69 avg=2.32\n",
            "[295 | 3074.42] loss=2.66 avg=2.32\n",
            "[296 | 3083.15] loss=2.25 avg=2.32\n",
            "[297 | 3091.85] loss=2.96 avg=2.33\n",
            "[298 | 3100.66] loss=1.97 avg=2.32\n",
            "[299 | 3109.38] loss=1.84 avg=2.32\n",
            "[300 | 3118.08] loss=2.61 avg=2.32\n",
            "Saving checkpoint/run1/model-300\n",
            "======== SAMPLE 1 ========\n",
            " and a simple one. In this paper, we analyze a variety of tasks related to understanding\n",
            "language. We build two new datasets: a corpus of 10,000 self-attention\n",
            "graphs (GPT-1s) and the corpus of 16,000 open-domain and text-only\n",
            "question-answering (ODA-QA) scores. Our analyses demonstrate that a\n",
            "self-attention score of at least 50% reproducibility is required\n",
            "for the best performance on the GPT-1 benchmark. Additionally, we find that it\n",
            "may take as little as 28-70% of the information to compute a self-attention score\n",
            "within the accuracy range of GPT. Finally, we quantify the value of such\n",
            "graphs from the human perspective. To facilitate the future development of\n",
            "such an instrument, we created charts and instructions to assist the\n",
            "experts of the NLP community in training such robots. This is the first\n",
            "research paper to tackle this large-scale dataset and our contribution is\n",
            "an extensive analysis of the models trained on it. We then ask the\n",
            "quandaries of self-attention score that still linger to this day: how\n",
            "easy to teach such a high level of information about self in a short amount\n",
            "of time; how safe can we make such a tool? Our model achieves high\n",
            "on both tests, but its inability to do so does not make it any less useful;\n",
            "so too does its lack of self-attention score for its lack that it cannot be\n",
            "adopted for more nuanced reasoning tasks that require deeper\n",
            "self-attention skills.\n",
            "[TITLES]:How Easy is it for Experts to Reason About Their Own Words?\n",
            "[ABSTRACT]:We present a unified framework for analysing ethical, legal, and social\n",
            "question-answering questions across various domains, topics, and time frames. This\n",
            "means that the entire literature of ethical, legal, and social-legal queries can\n",
            "be captured, analyzed, and analyzed in a unified manner by a single model.\n",
            "Our framework consists of three parts: a) a\n",
            "rule-based description model, b) a probabilistic model (BAM), c) a\n",
            "sentiment analysis model. Our model sifts the literature into four\n",
            "dimensions: ethics, legality, social, and economic. Within each dimension, our\n",
            "rule-based model identifies the ethical, legal, social, and economic dimensions of\n",
            "the query so that it can best reflect the context of the querying author\n",
            "and so make informed decisions based on the data they provide. Within\n",
            "each dimension, we analyze the model to assess how well the model\n",
            "explains the data given the given context, and so on.\n",
            "[TITLES]:Question Answering with a Uniform Form for Ethical, Legal, and Social-Legal Answers\n",
            "[ABSTRACT]:Although recent machine learning (ML) models have demonstrated great\n",
            "performance and have been widely adopted in the field of natural language\n",
            "processing and machine translation, they suffer from limited resources, limited\n",
            "ability to interactively consider the consequences of their models' decisions and\n",
            "not be able to express their subjective opinions. This limits their constructive\n",
            "actions towards a solution in difficult cases, but does not allow them to be\n",
            "instrumentally informed, and hence reflect more complex and nuanced\n",
            "concerns. Thus a constructive and helpful human is required to be able\n",
            "to analyze the model's reasoning and assess its possible\n",
            "disadvantages and opportunities. Human reasoning is still a challenging\n",
            "domain for human reasoning models to achieve human-like performance.\n",
            "To aid human thought processes, in this paper, we propose a set of human\n",
            "reasoning rules and a task-based language model known as rationally\n",
            "reasoning. Specifically, rationally reasoning models can reason\n",
            "with human-like reasoning ability, but are limited in practical\n",
            "practices of reasoning. We conducted extensive tests on\n",
            "social issues and human reasoning rules to explore potential gaps\n",
            "and inconsistencies in rationally reasoning models. We then proposed a solution\n",
            "based on both rationally reasoning methods and human thinking abilities\n",
            "and conducted extensive experiments to evaluate and discuss the current\n",
            "state of their state, as well as potential future achievements. We conclude\n",
            "that our proposed solution can significantly improve the human-based logic\n",
            "models' reasoning abilities and the human-based reasoning model's\n",
            "performance under difficult situations.\n",
            "[TITLES]:Human-based Rationality: Case Studies on the Challenges of Rationality and Reasoning\n",
            "[ABSTRACT]:The advancement of NLP models and natural language processing\n",
            "(NLP) has brought with it a surge in research on self-attention,\n",
            "which has become a crucial prerequisite for effective AI systems in fields such as\n",
            "research in medical, law, and technology-related fields. This work analyzes\n",
            "situational self-attention models to determine their strengths and\n",
            "weaknesses to facilitate clinical decision support from clinical\n",
            "leaders. We identify\n",
            "\n",
            "[301 | 3232.85] loss=1.82 avg=2.32\n",
            "[302 | 3241.52] loss=1.89 avg=2.31\n",
            "[303 | 3250.18] loss=2.88 avg=2.32\n",
            "[304 | 3258.73] loss=2.65 avg=2.32\n",
            "[305 | 3267.41] loss=2.72 avg=2.33\n",
            "[306 | 3276.04] loss=1.81 avg=2.32\n",
            "[307 | 3284.62] loss=2.32 avg=2.32\n",
            "[308 | 3293.28] loss=2.23 avg=2.32\n",
            "[309 | 3302.03] loss=2.79 avg=2.32\n",
            "[310 | 3310.62] loss=2.70 avg=2.33\n",
            "[311 | 3319.24] loss=1.82 avg=2.32\n",
            "[312 | 3327.85] loss=1.80 avg=2.32\n",
            "[313 | 3336.48] loss=1.60 avg=2.31\n",
            "[314 | 3345.18] loss=1.80 avg=2.30\n",
            "[315 | 3353.84] loss=2.76 avg=2.31\n",
            "[316 | 3362.47] loss=2.39 avg=2.31\n",
            "[317 | 3371.17] loss=1.33 avg=2.30\n",
            "[318 | 3379.84] loss=2.09 avg=2.30\n",
            "[319 | 3388.49] loss=2.30 avg=2.30\n",
            "[320 | 3397.22] loss=2.17 avg=2.30\n",
            "[321 | 3405.87] loss=1.94 avg=2.29\n",
            "[322 | 3414.53] loss=1.89 avg=2.29\n",
            "[323 | 3423.11] loss=2.67 avg=2.29\n",
            "[324 | 3431.80] loss=2.56 avg=2.30\n",
            "[325 | 3440.46] loss=2.05 avg=2.29\n",
            "[326 | 3449.01] loss=2.89 avg=2.30\n",
            "[327 | 3457.66] loss=1.41 avg=2.29\n",
            "[328 | 3466.39] loss=1.50 avg=2.28\n",
            "[329 | 3475.03] loss=1.71 avg=2.28\n",
            "[330 | 3483.64] loss=1.87 avg=2.27\n",
            "[331 | 3492.34] loss=1.81 avg=2.27\n",
            "[332 | 3501.00] loss=0.96 avg=2.25\n",
            "[333 | 3509.71] loss=2.75 avg=2.26\n",
            "[334 | 3518.41] loss=1.69 avg=2.25\n",
            "[335 | 3526.99] loss=1.96 avg=2.25\n",
            "[336 | 3535.55] loss=1.28 avg=2.24\n",
            "[337 | 3544.12] loss=1.70 avg=2.23\n",
            "[338 | 3552.79] loss=1.52 avg=2.23\n",
            "[339 | 3561.57] loss=2.13 avg=2.23\n",
            "[340 | 3570.19] loss=2.07 avg=2.22\n",
            "[341 | 3578.82] loss=1.26 avg=2.21\n",
            "[342 | 3587.45] loss=3.01 avg=2.22\n",
            "[343 | 3596.13] loss=2.66 avg=2.23\n",
            "[344 | 3604.79] loss=2.97 avg=2.23\n",
            "[345 | 3613.34] loss=1.50 avg=2.23\n",
            "[346 | 3621.99] loss=2.33 avg=2.23\n",
            "[347 | 3630.61] loss=1.49 avg=2.22\n",
            "[348 | 3639.22] loss=1.90 avg=2.22\n",
            "[349 | 3647.82] loss=2.34 avg=2.22\n",
            "[350 | 3656.39] loss=1.51 avg=2.21\n",
            "======== SAMPLE 1 ========\n",
            " is in keeping with what we have come to expect for many NLP tasks. We make our models publicly available on the OpenAI Research GitHub<|endoftext|>From Wikipedia, the free encyclopedia\n",
            "[1] The current research explores the use of open-source Python libraries to support various aspects of the medical data management tool,\n",
            "with a focus on the data privacy and confidentiality considerations.\n",
            "[1]:[ABSTRACT]:The demand for open-source tools has increased exponentially, contributing to the\n",
            "reliability, interoperability and transparency of medical data. One such tool is\n",
            "the Python interpreter, which can serve as a conduit through which Python applications can\n",
            "be executed. Recent work in open-source tools has focused on the command line interface\n",
            "and focused on large language models (LLMs). However, in the medical domain, it is currently\n",
            "the LLM interpretation tool, which interprets the LLM commands into command\n",
            "formats. We design a comprehensive approach to interpret the text of a medical\n",
            "command line tool into Python commands via a simple LLM. Our approach employs a\n",
            "command-writing grammar and a model library of several common LLMs, which\n",
            "generate command outputs for interpretation. With the LLM interpretation tool, we\n",
            "explore two popular use cases of Python commands, query-based queries, and\n",
            "evaluation of the resulting command-string results. To that end, we provide a\n",
            "comprehensive exploration of the available options including text and data\n",
            "mining, query reconstruction, and data-driven queries, all of which are\n",
            "related to the existing query engine in question answering (QA).\n",
            "In contrast to the results of LLMs, our approach focuses on the LLM's analysis\n",
            "and interpretation. We introduce two new query formats -- simple and complex --\n",
            "represent an exploration of alternative modes of analysis from three\n",
            "relevant LLMs for interpreting queries into command outputs. We find that the\n",
            "implicity in the LLM commands is enhanced by the fact that the LLM can easily be\n",
            "revised without changing the data representation. However, our code\n",
            "and experiment are hosted on PySpaces.\n",
            "[TITLES]:Using LLMs to Interpret Medical Query Outputs: An Exploration\n",
            "[ABSTRACT]:The field of NLP statistics has recently seen significant interest from\n",
            "state-of-the-art machine learning (ML) techniques, such as machine\n",
            "learning, natural language processing (NLP), and deep learning,\n",
            "including reinforcement learning. The purpose of this paper focuses on the\n",
            "field of statistics, particularly on the statistics from the statistical\n",
            "literature. The main ideas of the paper are: (1) analyzing and\n",
            "generating statistics from the statistical literature, and\n",
            "(2) generating statistical distributions from the distribution of the statistics\n",
            "literature. Furthermore, the paper proposes a comprehensive\n",
            "method for conducting meta-experiments comparing different methods of\n",
            "calculating a value. Finally, the paper presents a method for\n",
            "pre-calculating statistics from the distribution of the statistics\n",
            "literature. The presented method outperforms all prior methods on\n",
            "one particular study involving the use of the Rydberg-Karp estimator.\n",
            "[TITLES]:Statistical Explorations from a Distributing Approach to the Analysis of Statistics in the NLP-Statistics Community\n",
            "[ABSTRACT]:Aspective analysis (AOA) provides qualitative and/or quantitative analysis of\n",
            "the aspectives of a given sentence in English, which can be used for\n",
            "information extraction (IE) or translation (XMIT) processes.\n",
            "However, an AOA method needn't provide any additional information\n",
            "other than the annotation method to the annotation dataset. In this\n",
            "work, we propose a novel and affordable n-word annotation dataset (called\n",
            "NOO-NLU), which consists of several aspects and annotations, such as\n",
            "text and sentence length and context. The annotations also use\n",
            "language models and high-quality annotation data to make a more\n",
            "integral annotation of the aspective from the NL-talk mailing list\n",
            "(www.nlait-de.nlp.wa.ac.za/msg20160707.html). For the sake of\n",
            "understanding aspecties (aspectuities) and human evaluation (i.e., evaluation of\n",
            "an aspective's aspective), let's examine two examples of aspectiy analysis\n",
            "(both of which we will be submitting to the XMIT conference). Our first\n",
            "example involves a survey paper titled \"How many NLP systems are currently\n",
            "out there?\" We propose to perform aspectiy analysis on the two surveys\n",
            "published earlier this year (www.w3data.com/datasets/NS/NSANL/NSAN8.pdf) and\n",
            "published early this year (www.w3data.com/datasets/NS/NSANL/NSAN10.pdf).\n",
            "We explore three aspects (sentence length, context and sentence\n",
            "annotation types) of topic and survey paper\n",
            "\n",
            "[351 | 3769.69] loss=2.86 avg=2.22\n",
            "[352 | 3778.36] loss=1.73 avg=2.21\n",
            "[353 | 3787.26] loss=1.42 avg=2.20\n",
            "[354 | 3795.87] loss=2.59 avg=2.21\n",
            "[355 | 3804.52] loss=2.46 avg=2.21\n",
            "[356 | 3813.13] loss=2.28 avg=2.21\n",
            "[357 | 3821.81] loss=1.00 avg=2.20\n",
            "[358 | 3830.46] loss=2.73 avg=2.20\n",
            "[359 | 3839.26] loss=1.46 avg=2.20\n",
            "[360 | 3847.75] loss=1.32 avg=2.19\n",
            "[361 | 3856.26] loss=1.86 avg=2.18\n",
            "[362 | 3864.86] loss=1.50 avg=2.18\n",
            "[363 | 3873.36] loss=1.78 avg=2.17\n",
            "[364 | 3881.91] loss=1.00 avg=2.16\n",
            "[365 | 3890.66] loss=0.85 avg=2.15\n",
            "[366 | 3899.39] loss=2.59 avg=2.15\n",
            "[367 | 3907.95] loss=2.61 avg=2.16\n",
            "[368 | 3916.48] loss=1.72 avg=2.15\n",
            "[369 | 3925.12] loss=1.71 avg=2.15\n",
            "[370 | 3933.63] loss=1.12 avg=2.14\n",
            "[371 | 3942.24] loss=1.31 avg=2.13\n",
            "[372 | 3951.05] loss=1.99 avg=2.13\n",
            "[373 | 3959.61] loss=1.75 avg=2.12\n",
            "[374 | 3968.22] loss=1.67 avg=2.12\n",
            "[375 | 3976.79] loss=1.63 avg=2.11\n",
            "[376 | 3985.35] loss=1.31 avg=2.11\n",
            "[377 | 3993.96] loss=2.45 avg=2.11\n",
            "[378 | 4002.53] loss=1.16 avg=2.10\n",
            "[379 | 4011.05] loss=2.25 avg=2.10\n",
            "[380 | 4019.54] loss=1.30 avg=2.09\n",
            "[381 | 4028.00] loss=1.33 avg=2.09\n",
            "[382 | 4036.57] loss=1.05 avg=2.08\n",
            "[383 | 4045.18] loss=1.38 avg=2.07\n",
            "[384 | 4053.71] loss=1.80 avg=2.07\n",
            "[385 | 4062.26] loss=2.07 avg=2.07\n",
            "[386 | 4070.89] loss=1.86 avg=2.06\n",
            "[387 | 4079.46] loss=2.68 avg=2.07\n",
            "[388 | 4088.06] loss=3.07 avg=2.08\n",
            "[389 | 4096.50] loss=2.02 avg=2.08\n",
            "[390 | 4104.97] loss=1.53 avg=2.07\n",
            "[391 | 4113.37] loss=1.85 avg=2.07\n",
            "[392 | 4121.82] loss=2.04 avg=2.07\n",
            "[393 | 4130.31] loss=2.41 avg=2.07\n",
            "[394 | 4138.78] loss=2.49 avg=2.08\n",
            "[395 | 4147.31] loss=1.54 avg=2.07\n",
            "[396 | 4155.91] loss=1.40 avg=2.07\n",
            "[397 | 4164.46] loss=1.53 avg=2.06\n",
            "[398 | 4172.90] loss=2.21 avg=2.06\n",
            "[399 | 4181.28] loss=2.01 avg=2.06\n",
            "[400 | 4189.75] loss=1.19 avg=2.05\n",
            "Saving checkpoint/run1/model-400\n",
            "======== SAMPLE 1 ========\n",
            "p+\",\"is\",\"is not\"],\"types\":[\"java.lang.Number\"]},{\"value\":\"\\\"Modified Date\\\"\",\"displayName\":\"Modified Date - cf[10408]\",\"auto\":\"true\",\"orderable\":\"true\",\"searchable\":\"true\",\"operators\":[\"=\\in\\s2|eth-cap-date|=\\in\\s7|\"]},\"types\":[\"CS\"]},{\"value\":\"Name\",\"displayName\":\"Name\",\"text\":\"Name\",\"indices\":[1,12],\"types\":[\"com.atlassian.jira.project.ProjectName\"]},{\"value\":\"OpenPGP Key\",\"displayName\":\"OpenPGP Key - cf[10604]\",\"auto\":\"true\",\"orderable\":\"true\",\"searchable\":\"true\",\"operators\":[\"=\\in\\s2|eth-key\",\"is\",\"is not\"],\"types\":[\"com.atlassian.jira.issue.asc.Accessibility\"]},{\"value\":\"password\",\"displayName\":\"password\",\"auto\":\"true\",\"searchable\":\"true\",\"operators\":[\"=\\thepublic\\@\\in\\the\\of\\\",\"!=\\thepublic\\@{$1}$\\in\\text$}\\in\\textit$}$\",\"types\":[\"com.atlassian.jira.issue.password.Secure\"],\"settings\":\"order in which to display passwords is governed by a set of rules.\",\"zoomLevels\":[{\"value\":\"Medium\",\"table\":[0]=>{}}, \"medium\":{}, \"large\":{}, \"small\":{}, \"-2x\":{}}, \"}}}\n",
            "-[[$:/core/wiki/Design.current.work\",\"jumplists\"],\"text\":\"More\n",
            "\n",
            "(internal). More information about design decisions is available at design-dies-ed. See\n",
            "http://en.wikisource.org/wiki/Design_decision\".\n",
            "[TITLES]:The Design Decision Engine: Knowledge Discovery for Open Source Systems\n",
            "[ABSTRACT]:With good reason, the search for appropriate synthetic performance metrics and automated\n",
            "tooling are complex engineering challenges. Unfortunately, the large quantity of open-source\n",
            "documents makes the task of performing manual annotation of these metrics relatively\n",
            "simple. In this paper, we have introduced a new task called Automatic Evaluation of NLP\n",
            "metrics for Open Source Synthetic Benchmarks that exploits this ambiguity, where we first\n",
            "introduce a new annotation scheme consisting of two components -- manual testing of\n",
            "syntactic and semantic parsing in a simulated OpenAI environment and an automatic\n",
            "evaluation of annotation results in an input-return evaluation framework. To achieve\n",
            "this, we use a novel annotation paradigm, based on a pre-trained Convolutional Neural\n",
            "Networks model as an automatic annotation dataset, and then derive annotator character\n",
            "proportions from public data. Subsequently, we train a classifier model on input-return\n",
            "evaluation results and compare to historical benchmark result sets. We find that current manual\n",
            "annotations frameworks still deliver unsatisfactory results compared to our\n",
            "automated benchmarks, despite adopting a similar structure. Further investigations\n",
            "reveal important bottlenecks in existing metrics in that the model's performance\n",
            "suffers from insufficient sample sizes. This highlights the need for a\n",
            "smaller and more accessible set of raw benchmarks with better annotation\n",
            "and evaluation metrics.\n",
            "[TITLES]:AutoEx for Open Source Synthetic Benchmarks: Automatic Evaluation of NLP Metrics for Open Source Systems\n",
            "[ABSTRACT]:This paper has drawn a lot of attention because of its key insights into the\n",
            "science of climate change. In particular, we have discovered that, in the last\n",
            "20 years, the average warmth increase observed for the Earth's surface to the\n",
            "tenth percentile of the Holocene was 2.7 Cº , making this the warmest (relative)\n",
            "temperature increase on Earth. Our research has been motivated by a desire to\n",
            "explore and contribute to future developments in interpreting trends in global\n",
            "climate data in future periods of time. With this aim, we have chosen to\n",
            "conduct our study in the Climate Climate Observed from Space Collection (CCOCOC)\n",
            "at the NIS 2 level. Because it represents the basic climate dataset, CCOC\n",
            "allows for the evaluation of both climate scenarios -- temperature\n",
            "instead of precipitation (rather than absolute amounts); and for climate\n",
            "in terms of how the atmosphere behaves at different spatial scales. Through this\n",
            "study, we have established the generalization abilities of the current\n",
            "natural language processing (NLP) models to climate data. Also,\n",
            "CCOC enables for further development and application of climate related\n",
            "models in general and for different climates; thus, CCOC offers as\n",
            "a start a promising approach for future research in interpreting climate\n",
            "data on various scales. In the end, however, CCOC ultimately presents a\n",
            "challenge to various downstream climate prediction systems as well as the general\n",
            "nature of what information it provides at different scales. In this work, we have\n",
            "\n",
            "[401 | 4304.30] loss=1.81 avg=2.05\n",
            "[402 | 4312.81] loss=2.61 avg=2.06\n",
            "[403 | 4321.39] loss=1.42 avg=2.05\n",
            "[404 | 4330.00] loss=1.49 avg=2.04\n",
            "[405 | 4338.52] loss=2.22 avg=2.05\n",
            "[406 | 4347.07] loss=1.26 avg=2.04\n",
            "[407 | 4355.64] loss=1.33 avg=2.03\n",
            "[408 | 4364.19] loss=3.04 avg=2.04\n",
            "[409 | 4372.74] loss=1.46 avg=2.04\n",
            "[410 | 4381.21] loss=0.83 avg=2.02\n",
            "[411 | 4389.76] loss=3.12 avg=2.03\n",
            "[412 | 4398.27] loss=1.62 avg=2.03\n",
            "[413 | 4406.84] loss=3.37 avg=2.04\n",
            "[414 | 4415.37] loss=1.93 avg=2.04\n",
            "[415 | 4423.89] loss=1.67 avg=2.04\n",
            "[416 | 4432.48] loss=3.02 avg=2.05\n",
            "[417 | 4441.12] loss=1.94 avg=2.05\n",
            "[418 | 4449.59] loss=1.14 avg=2.04\n",
            "[419 | 4458.10] loss=1.57 avg=2.03\n",
            "[420 | 4466.69] loss=2.37 avg=2.04\n",
            "[421 | 4475.21] loss=1.52 avg=2.03\n",
            "[422 | 4483.73] loss=1.61 avg=2.03\n",
            "[423 | 4492.33] loss=1.39 avg=2.02\n",
            "[424 | 4500.83] loss=2.20 avg=2.02\n",
            "[425 | 4509.22] loss=2.20 avg=2.02\n",
            "[426 | 4517.76] loss=1.62 avg=2.02\n",
            "[427 | 4526.27] loss=2.09 avg=2.02\n",
            "[428 | 4534.74] loss=1.91 avg=2.02\n",
            "[429 | 4543.32] loss=1.46 avg=2.01\n",
            "[430 | 4551.94] loss=1.98 avg=2.01\n",
            "[431 | 4560.52] loss=1.63 avg=2.01\n",
            "[432 | 4569.12] loss=1.61 avg=2.01\n",
            "[433 | 4577.62] loss=2.66 avg=2.01\n",
            "[434 | 4586.11] loss=1.78 avg=2.01\n",
            "[435 | 4594.67] loss=2.27 avg=2.01\n",
            "[436 | 4603.23] loss=0.90 avg=2.00\n",
            "[437 | 4611.85] loss=3.05 avg=2.01\n",
            "[438 | 4620.43] loss=0.64 avg=2.00\n",
            "[439 | 4628.89] loss=1.64 avg=1.99\n",
            "[440 | 4637.44] loss=2.38 avg=2.00\n",
            "[441 | 4645.94] loss=1.86 avg=2.00\n",
            "[442 | 4654.62] loss=0.91 avg=1.99\n",
            "[443 | 4663.12] loss=1.20 avg=1.98\n",
            "[444 | 4671.58] loss=1.15 avg=1.97\n",
            "[445 | 4680.08] loss=2.63 avg=1.98\n",
            "[446 | 4688.58] loss=1.28 avg=1.97\n",
            "[447 | 4697.13] loss=0.84 avg=1.96\n",
            "[448 | 4705.66] loss=1.76 avg=1.96\n",
            "[449 | 4714.23] loss=0.82 avg=1.94\n",
            "[450 | 4722.82] loss=1.12 avg=1.94\n",
            "======== SAMPLE 1 ========\n",
            " performance and\n",
            "effectiveness of the proposed approaches. In addition, we discuss the\n",
            "advantages and limitations of the methods, the future of the field, and\n",
            "the future research direction of NLP for tuberculosis. Finally, we close by\n",
            "prompting participants to review their work online: https://osu.go…/how-to-report-tuberculosis\n",
            "[TITLES]:How to Report a TB Case: A Report Walkthrough, Exploring Evaluation and Detection Methods\n",
            "[ABSTRACT]:The rapid spread and growth of information technologies in the humanities and social\n",
            "sciences has spurred a surge of research in this area. Among the many initiatives\n",
            "active in this area, the largest is the Digital Humanities+Verbal Learning\n",
            "(DHV) Consortium, which comprises 35 member institutions, including Boston University,\n",
            "Cornell University, Dartmouth College, University of Michigan, University of Pennsylvania,\n",
            "University of Texas, University of British Columbia, University of Western Ontario, and\n",
            "University of Western Australia. Through a series of 6 conferences in 2013–14,\n",
            "the Consortium established a Data and Methodology Center at the BERNSTEIN Conference on\n",
            "Verification, which aims to foster research and development of tools and\n",
            "methods to enhance the data and workflow of DHV. In September\n",
            "2014, the Consortium announced the successful completion of the Boston University Phase\n",
            "of the DHV Consortium Research Agenda. The plan is to release all the code and\n",
            "data for the first time at the New York DHV Conference on Verification on 30th November,\n",
            "2020 to kick-start future developments. To date, the DHV Consortium has\n",
            "released the following tools and resources:\n",
            "* Boston University Dataset - BERT: trained models for quantitative evaluation\n",
            "* Cornell Data Sets - CTRCT: pre-trained language models for multilingual\n",
            "* University of Michigan Data Sets - UMT: short texts with multimodal\n",
            "* University of British Columbia Data Sets - UBC: long texts with multimodal\n",
            "* Cornell Audio-Verse Lexicon - ACCE: pre-trained lexicon of college sports\n",
            "* University of British Columbia Data Sets - UBC: soundsystem-quality\n",
            "[TITLES]:Boston University Phase of the DHV Consortium for Research into Multilingual Evaluation and Detection of\n",
            "Verbal Inertia in Social Text\n",
            "[ABSTRACT]:The field of behavioral economics has recently seen a surge in research efforts on\n",
            "social psychology of human behavior, particularly in the areas of interpersonal\n",
            "personalization, environmental threats, and the misuse of power. In recent times,\n",
            "a large body of empirical work has been conducted on human behavior in\n",
            "these fields, often employing Large Language Models (LLMs). Despite the\n",
            "success of these models, they are unable to fully capture complex\n",
            "systematic aspects of human behavioral psychology, such as normative\n",
            "underpinnings and facilitators. In this paper, we introduce a novel\n",
            "human-in-the-Loop (HICL) model, capable of exploring normative aspects of\n",
            "human behavior through the platform of human-in-the-Workspace (HFIC). Specifically,\n",
            "HICL captures information from human teachers and students via a\n",
            "multilingually compatible Human-In-the-Workspace (HITW) platform and deploys\n",
            "it as a Large Language Model, which can be piloted automatically in\n",
            "high-stakes settings and which elicits valid teacher responses. We perform\n",
            "extensive experiments using HICL to explore normative aspects of behavior, and\n",
            "show that such insights are possible through the lens of HITW. Furthermore, we\n",
            "extensively evaluate several existing large language models on the HITW\n",
            "question-answering task and demonstrate that only HICL emerges as the clear\n",
            "face of the T5 on a tie-breaker basis, while other LMs perform below\n",
            "speedy (SST-HiP). Our experimental results show that HICL outperforms existing large\n",
            "language models in both the HICCI and HITW tasks, especially in the low-stakes\n",
            "settings. Our code can be found at:\n",
            "https://hicl.github.io/HICl-Tool-Scenarios/HICl-HIClT5.\n",
            "[TITLES]:HICl-HIClT5: Large Language Model for Human In-The-Workspace Human-In-The-Workspace\n",
            "[ABSTRACT]:Sentence embedding is one of the most studied topics in Natural Language\n",
            "Processing. Despite the impressive potential of natural language\n",
            "processing (NLP) in various tasks, most of its applications are still\n",
            "applied using manual intervention over a data source, which can be\n",
            "time-consuming and expensive. This paper aims to combine the power of NLP\n",
            "and machine learning to empower natural language processing (NLP) applications\n",
            "through a training framework that automatically selects the most relevant examples\n",
            "and accomplishes a task using an ongoing learning framework.\n",
            "\n",
            "[451 | 4835.79] loss=0.89 avg=1.93\n",
            "[452 | 4844.57] loss=1.12 avg=1.92\n",
            "[453 | 4853.29] loss=0.69 avg=1.91\n",
            "[454 | 4861.91] loss=3.22 avg=1.92\n",
            "[455 | 4870.57] loss=0.62 avg=1.91\n",
            "[456 | 4879.21] loss=0.62 avg=1.89\n",
            "[457 | 4887.75] loss=1.11 avg=1.88\n",
            "[458 | 4896.43] loss=1.15 avg=1.88\n",
            "[459 | 4905.00] loss=2.34 avg=1.88\n",
            "[460 | 4913.54] loss=1.83 avg=1.88\n",
            "[461 | 4922.17] loss=0.83 avg=1.87\n",
            "[462 | 4930.74] loss=1.59 avg=1.87\n",
            "[463 | 4939.45] loss=2.68 avg=1.88\n",
            "[464 | 4948.14] loss=1.98 avg=1.88\n",
            "[465 | 4956.80] loss=0.53 avg=1.86\n",
            "[466 | 4965.51] loss=1.82 avg=1.86\n",
            "[467 | 4974.13] loss=1.26 avg=1.86\n",
            "[468 | 4983.05] loss=1.98 avg=1.86\n",
            "[469 | 4991.72] loss=2.26 avg=1.86\n",
            "[470 | 5000.38] loss=2.49 avg=1.87\n",
            "[471 | 5009.27] loss=0.77 avg=1.86\n",
            "[472 | 5017.87] loss=0.84 avg=1.85\n",
            "[473 | 5026.69] loss=2.55 avg=1.85\n",
            "[474 | 5035.39] loss=0.90 avg=1.84\n",
            "[475 | 5044.06] loss=1.09 avg=1.84\n",
            "[476 | 5052.88] loss=0.64 avg=1.83\n",
            "[477 | 5061.68] loss=0.98 avg=1.82\n",
            "[478 | 5070.47] loss=0.99 avg=1.81\n",
            "[479 | 5079.19] loss=1.67 avg=1.81\n",
            "[480 | 5087.85] loss=2.18 avg=1.81\n",
            "[481 | 5096.43] loss=1.10 avg=1.80\n",
            "[482 | 5105.14] loss=2.41 avg=1.81\n",
            "[483 | 5113.77] loss=1.68 avg=1.81\n",
            "[484 | 5122.43] loss=1.83 avg=1.81\n",
            "[485 | 5131.10] loss=2.03 avg=1.81\n",
            "[486 | 5139.86] loss=1.78 avg=1.81\n",
            "[487 | 5148.60] loss=1.15 avg=1.80\n",
            "[488 | 5157.23] loss=2.76 avg=1.81\n",
            "[489 | 5165.90] loss=1.68 avg=1.81\n",
            "[490 | 5174.57] loss=2.37 avg=1.82\n",
            "[491 | 5183.29] loss=1.99 avg=1.82\n",
            "[492 | 5192.00] loss=3.05 avg=1.83\n",
            "[493 | 5200.69] loss=2.47 avg=1.84\n",
            "[494 | 5209.35] loss=2.17 avg=1.84\n",
            "[495 | 5218.25] loss=2.28 avg=1.85\n",
            "[496 | 5227.06] loss=2.01 avg=1.85\n",
            "[497 | 5235.69] loss=1.53 avg=1.84\n",
            "[498 | 5244.39] loss=1.82 avg=1.84\n",
            "[499 | 5252.99] loss=1.52 avg=1.84\n",
            "[500 | 5261.73] loss=2.05 avg=1.84\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ],
      "source": [
        "gpt2.finetune(sess,\n",
        "              'chatbot.txt',\n",
        "              model_name='355M',\n",
        "              steps=500, #200\n",
        "              run_name='run1',\n",
        "              save_every=100,\n",
        "              sample_every=50,\n",
        "              #use_memory_saving_gradients = True,\n",
        "\t            only_train_transformer_layers = True,\n",
        "            \taccumulate_gradients = 1)\n",
        "              #reuse=True)   # steps is max number of training steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv7Iwx1XDUfv"
      },
      "source": [
        "###Generate Text From The Trained Model\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. generate generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb71QaV-DZAB",
        "outputId": "068057b9-1d23-4fb0-c0e2-0d192869ceab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This survey is the first attempt to investigate the\n",
            "potential of stories to enhance student learning. We conduct a\n",
            "comparative analysis of a wide range of narrative story types, including\n",
            "symbolic narratives, true stories, mini-sites, and narratives that\n",
            "follow a prescribed narrative structure. We use a novel data\n",
            "format that allows for an examination of the understanding and reasoning\n",
            "capabilities of students, and focus on questions such as:\n",
            "\"What is the main difference between a narrative story and a true story?\"\n",
            "\"Does the use of a story make a difference in student performance?\" and \"How\n",
            "do students think stories affect them?\"\n",
            "The answers to these questions will facilitate further research in\n",
            "this area of interest. We provide three case studies to illustrate the\n",
            "effectiveness of using stories in learning. In addition, we offer case studies\n",
            "that examine the impact of language interventions in enhancing student\n",
            "learning performance. The case study focuses on the understanding and reasoning\n",
            "capabilities of students and examines the impact of language elements\n",
            "such as narration and interjections in enhancing the student's\n",
            "reasoning abilities. In addition, we analyze the impact of story elements\n",
            "such as narration, interjections, and narration of narrative elements\n",
            "(NINR) on student performance. We conclude our case study with a discussion of\n",
            "the potential of story elements to enhance student reasoning.\n",
            "[TITLES]:How Does Student Reason About Stories?\n",
            "[ABSTRACT]:The digital world is now saturated with ads, and publishers are\n",
            "inequitably eager to capitalize on the opportunity. They\n",
            "seek to leverage natural language processing (NLP) techniques to\n",
            "preserve relevant knowledge and perform statistics, including\n",
            "advancements in digitised text recognition and machine learning\n",
            "(ML) techniques. Despite the promising potential of NLP techniques,\n",
            "advancements like machine learning (ML) techniques are typically\n",
            "introduced via a series of specialized benchmarks, and often demand\n",
            "specialised input from the test taker. This poses a significant\n",
            "challenge for NLP techniques to advance towards the more\n",
            "reliable and accurate task of predicting the ads of a given site.\n",
            "We propose to address this issue by providing a user-friendly\n",
            "benchmark that encourages the user to identify the ads of a given\n",
            "site using simple text classification. We define a novel approach,\n",
            "advancement barcode scanning, to help promote the use of NLP\n",
            "techniques for real-time decision-making. In addition, we provide a\n",
            "comparative analysis of two popular approaches for rapid ad discovery,\n",
            "NLP techniques and ML techniques. Our primary findings reveal that\n",
            "advancement barcode scanning offers significant advantages over\n",
            "traditional ML techniques, and can significantly reduce ad-ad\n",
            "reconstruction time. We also find that although NLP techniques can\n",
            "improve upon traditional ML techniques in terms of ad-ad\n",
            "reconstruction time, NLP techniques require significant more computing\n",
            "power and memory to train. Our proposed solution enables meaningful\n",
            "ad-ad reconfiguration by leveraging the unique patterns of text\n",
            "classification and ad-ad pairing. We demonstrate that our proposed\n",
            "approach outperforms both single-threaded and multi-threaded ML\n",
            "techniques, and generally outperforms local language models when\n",
            "computationally intensive (i.e., >100 million words).\n",
            "[TITLES]:Advancement Barcode Scanning\n",
            "[ABSTRACT]:In this paper, we present a novel framework for interpreting\n",
            "intersection models's behavior in relation to previous knowledge and\n",
            "current knowledge. We focus on the task of predicting the interplay\n",
            "between a series of knowledge bases (e.g., human language,\n",
            "i.e., the user types in a query, what to expect from the Knowledge Base).\n",
            "Prior work in NLP has primarily focused on the task of generating\n",
            "an estimate of the overlap between two knowledge bases, which has failed to\n",
            "take into account the fact that the model may have generated\n",
            "knowledge bases that have different causal effects on the same query. We\n",
            "improperly assume that the model has known how to generate\n",
            "knowledge bases, an assumption that can be easily refuted. Instead, we\n",
            "propose a framework for generating knowledge bases based on the model's\n",
            "pre-training knowledge using a new supervised prefiltering layer and a new\n",
            "supervised prefiltering layer. This model is trained using GPT-based\n",
            "pre-training and fine-tuned using GPT-based fine-tuning, and then fine-tuned\n",
            "using a special Kaggle -based pre-training dataset. We show that the model and its\n",
            "supervised pre-training set-up outperform existing state-of-the-art\n",
            "Kaggle-based pre-training. In addition to the model, we introduce a new supervised\n",
            "pre-training method, which improves the model's performance over prior work.\n",
            "[TITLES]:Forecasting Knowledge bases using Knowledge Base Pre-\n"
          ]
        }
      ],
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6pdENBDcLG"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do text = gpt2.generate(sess, return_as_list=True)[0]\n",
        "\n",
        "You can also pass in a prefix to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing nsamples. Unique to GPT-2, you can pass a batch_size to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for batch_size).\n",
        "\n",
        "Other optional-but-helpful parameters for gpt2.generate and friends:\n",
        "\n",
        "* length: Number of tokens to generate (default 1023, the maximum)\n",
        "* temperature: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* top_k: Limits the generated guesses to the top k guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set top_k=40)\n",
        "* top_p: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "* truncate: Truncates the input text until a given sequence, excluding that sequence (e.g. if truncate='<|endoftext|>', the returned text will include everything before the first <|endoftext|>). It may be useful to combine this with a smaller length if the input texts are short.\n",
        "* include_prefix: If using truncate and include_prefix=False, the specified prefix will not be included in the returned text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj5jMzUBphSi"
      },
      "source": [
        "Run this cell to initialize or clear the new_titles array.....Whenever you're generating titles for a new abstract run this cell first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vsixr9aUJjvV"
      },
      "outputs": [],
      "source": [
        "def listToString(s):\n",
        "  str1 = \" \"\n",
        "  return (str1.join(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urepjN15c24F",
        "outputId": "d2e3dd11-64c4-4997-ef1d-5f247b0801a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abstract? The quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive study of how the quality of embeddings changes according to these features. Apart from identifying the most influential hyper-parameters, we also observe one that creates contradictory results between intrinsic and extrinsic evaluations. Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings.\n"
          ]
        }
      ],
      "source": [
        "new_titles=[]\n",
        "a=input('Abstract? ')\n",
        "inp='\\n'+'[ABSTRACT]:'+a+'\\n'+'[TITLE]:'\n",
        "for _ in range(5):\n",
        "  t=gpt2.generate(sess,\n",
        "                length=40,\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                top_p=0.8,\n",
        "                prefix=inp,\n",
        "                nsamples=1,\n",
        "                return_as_list=True)\n",
        "\n",
        "  t1=listToString(t).split('\\n')\n",
        "  for y in range(len(t1)):\n",
        "    if(y%2!=0):\n",
        "      x = t1[y].replace('[TITLE]:','').lower()\n",
        "      new_titles.append(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5HdMrzWOPeI",
        "outputId": "498115db-3605-45bd-b6d3-461c2143879c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a comprehensive study of word embeddings in nlp and medical nlp\n",
            "what's in a name? evaluating the quality of word embeddings\n",
            "embedding quality and the state of the art of nlp\n",
            "how does word embedding quality affect the biomedical domain word embeddings?\n",
            "what is the best word embedding for biomedical?\n"
          ]
        }
      ],
      "source": [
        "for x in new_titles:\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOdGZ7cRpC5c"
      },
      "source": [
        "ot=Original title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PRJCSlmggaj",
        "outputId": "cf240f5a-0f76-4fca-abea-d0c2963c18ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': Score(precision=0.36363636363636365, recall=0.4444444444444444, fmeasure=0.39999999999999997), 'rougeL': Score(precision=0.36363636363636365, recall=0.4444444444444444, fmeasure=0.39999999999999997)}\n"
          ]
        }
      ],
      "source": [
        "t=\"how does word embedding quality affect the biomedical domain word embeddings?\".lower()\n",
        "ot=\"How to Train Good Word Embeddings for Biomedical NLP\".lower()\n",
        "scorer=rouge_scorer.RougeScorer(['rouge1','rougeL'],use_stemmer=True)\n",
        "#for i in range(len(new_titles)):\n",
        "print(scorer.score(ot,t))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
